
@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}

@misc{liang_holistic_2023,
	title = {Holistic Evaluation of Language Models},
	url = {http://arxiv.org/abs/2211.09110},
	doi = {10.48550/arXiv.2211.09110},
	abstract = {Language models ({LMs}) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models ({HELM}) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for {LMs}. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5\% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream {LM} evaluation. Prior to {HELM}, models on average were evaluated on just 17.9\% of the core {HELM} scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for {HELM} to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
	number = {{arXiv}:2211.09110},
	publisher = {{arXiv}},
	author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and Ré, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
	urldate = {2024-03-12},
	date = {2023-10-01},
	eprinttype = {arxiv},
	eprint = {2211.09110 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/giskard/Zotero/storage/NWELZ2RU/Liang et al. - 2023 - Holistic Evaluation of Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/giskard/Zotero/storage/CE33BV3J/2211.html:text/html},
}

@online{poktscan_open_nodate,
	title = {{OPEN}: {POKT} {AI} Lab --- Socket - Build / Quick Grants},
	url = {https://forum.pokt.network/t/open-pokt-ai-lab-socket/5056/4},
	shorttitle = {{OPEN}},
	abstract = {Assemble with us to share ideas, help each other, and shape the future of our ecosystem.},
	titleaddon = {Pocket Network Forum},
	author = {Poktscan, Rawthil},
	urldate = {2024-03-12},
	langid = {english},
	file = {Snapshot:/home/giskard/Zotero/storage/GTXY7DVH/5056.html:text/html},
}

@misc{gao_framework_2023,
	title = {A framework for few-shot language model evaluation},
	url = {https://zenodo.org/records/10256836},
	publisher = {Zenodo},
	author = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and {DiPofi}, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and {McDonell}, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
	date = {2023-12},
	doi = {10.5281/zenodo.10256836},
	note = {Version Number: v0.4.0},
}


@misc{cloudflareLimitsCloudflare,
	author = {},
	title = {{L}imits · {C}loudflare {W}orkers docs --- developers.cloudflare.com},
	url = {https://developers.cloudflare.com/workers/platform/limits/#request-limits},
	year = {},
	note = {Accessed 22-03-2024},
}


@misc{openAI_API,
	author = {Open AI},
	title = {LLM - API Reference},
	url = {https://platform.openai.com/docs/api-reference},
	year = {},
	note = {Accessed 22-03-2024},
}

@misc{generative_ai_market,
	author = {IOT Analytics},
	title = {The leading generative AI companies},
	url = {https://iot-analytics.com/leading-generative-ai-companies/},
	year = {},
	note = {Accessed 22-03-2024},
}

@misc{hf_diffusers,
	author = {Hugging Face},
	title = {Diffusers},
	url = {https://huggingface.co/docs/diffusers/index},
	year = {},
	note = {Accessed 22-03-2024},
}


@misc{StableDiffusion_API,
	author = {Stable Diffusion},
	title = {API Reference},
	url = {https://stablediffusionapi.com/docs/},
	year = {},
	note = {Accessed 22-03-2024},
}


@misc{allora_litepaper,
	author = {Diederik Kruijssen and Emmons, Nicholas and Peluso, Kenneth and Ghaffar, Faisal and Huang, Alexander},
	title = {Allora: a Self-Improving, Decentralized Machine Intelligence Network},
	url = {https://litepaper.assets.allora.network/allora-litepaper.pdf},
	year = {2024},
}




@article{paszke2017automatic,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}


@article{kalamkar2019study,
  title={A study of BFLOAT16 for deep learning training},
  author={Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and others},
  journal={arXiv preprint arXiv:1905.12322},
  year={2019}
}

@article{lin2023awq,
  title={Awq: Activation-aware weight quantization for llm compression and acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{guan2024aptq,
  title={APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models},
  author={Guan, Ziyi and Huang, Hantao and Su, Yupeng and Huang, Hong and Wong, Ngai and Yu, Hao},
  journal={arXiv preprint arXiv:2402.14866},
  year={2024}
}

@article{petrov2024language,
  title={Language model tokenizers introduce unfairness between languages},
  author={Petrov, Aleksandar and La Malfa, Emanuele and Torr, Philip and Bibi, Adel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{li2022survey,
  title={A survey on retrieval-augmented text generation},
  author={Li, Huayang and Su, Yixuan and Cai, Deng and Wang, Yan and Liu, Lemao},
  journal={arXiv preprint arXiv:2202.01110},
  year={2022}
}


@article{goodfellow2020generative,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{nash1950equilibrium,
  title={Equilibrium points in n-person games},
  author={Nash Jr, John F},
  journal={Proceedings of the national academy of sciences},
  volume={36},
  number={1},
  pages={48--49},
  year={1950},
  publisher={National Acad Sciences}
}


@book{russell2016artificial,
  title={Artificial intelligence: a modern approach},
  author={Russell, Stuart J and Norvig, Peter},
  year={2016},
  publisher={Pearson}
}

@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={8780--8794},
  year={2021}
}



