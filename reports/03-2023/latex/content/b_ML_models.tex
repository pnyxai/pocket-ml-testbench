\newacronym{AI}{AI}{Artificial Intelligence}
\newacronym{ML}{ML}{Machine Learning}

\section{Machine Learning Models}\label{sec:ref}

Machine Learning is a wide term that conflates many different kinds of mathematical models. In this section we pretend to present their main distinctions in terms of the methods used to test or train them. This distinction is very important to the Pocket Network, since one of the main challenges of the protocol is to asses the correctness of the models inference. 

For those not familiar with the terminology, the terms Artificial Intelligence (AI) and Machine Learning (ML) are not interchangeable. The first one refers to systems that perform tasks that require human intelligence and the second one is a subset of AI which contains algorithms that are capable of learning from data. The later is the correct one to use in the context of the models served by the Pocket Network.

\subsection{Supervised Models - Ground Truth}
Supervised models are the subset of ML models that are trained on tasks that have a specific goal. For instance, an algorithm that tries to predict the movements of the financial market is a supervised model. That model has a specific task, given some inputs (any kind, lets say previous prices and market volumes), predict if an asset will increase or decrease its value. During training the model will be presented with known pairs of inputs-outputs and the model output will be compared to the objective using a hard metric like quadratic distance.
The performance of these models is measured in the same way that they are trained, only that for testing the model is given sets of inputs that it has never seen before.

In the case of supervised models the ground truth is always present, or it will be revealed at some point in time. In our financial market movements example, we just need to wait and see what happens. So, if we want to test the quality of these models there is a mathematical way to do it and the data is or will be available. 

These kinds of models are easy to test on production and their quality can be measured easily on the Pocket Network, we just need to feed them test samples and check the output error, or even create simple majority errors and punish outliers~\footnote{The Allora network has a great lite paper on a blockchain based implementation for these kind of models, see~\ref{allora_litepaper}. }.

Examples of these models are:
\begin{itemize}
    \item Linear Regression (predict prices of an asset).
    \item Classification (by any technique, like neural networks or support vector machines).
    \item Decision Trees (credit risk assessments).
    \item Convolutional Neural Networks (image segmentation)
\end{itemize}

\subsection{Unsupervised Models - Structure Discovery}

Another type of ML models are those models that do not need a human annotated dataset or a absolute truth value to be trained. The training sets are only composed of inputs to the network, examples with no tags or questions without answers, and hence the training is done in an unsupervised fashion, no metric is \emph{directly} guiding the output of the model.

These kind of models are normally used to discover patterns in data (clustering), like dividing large groups of data. An example of these kind of models are the natural language embeddings, these models take as input a string of characters and produce an output that is a codified vector of floats. The output of the model is actually a point in a multidimensional space, and its neighbors (other points close to it) are projections of other strings that have the same meaning or talk about the same subject. 
The natural language embeddings (like ROBERTA~\cite{liu2019roberta}) are used in Retrieval Augmented Generation systems (RAGs)~\cite{li2022survey}, when these RAGs need to find context for a query they first pass the input string to one of these models and retrieve its codification and then they use a vector database to find other strings in the vicinity of the input's code. Then they read the data in the neighboring codes and produce an output based on those.

Unsupervised models like encoders used for RAGs can be the easiest models to support in the Pocket Network, they are like blockchains in a sense. A given query should always return \emph{exactly} the same output, so a simple majority voting or challenge mechanism can be used to prove the data integrity. Other models that perform free-structure discovery (like clustering) could require further analysis (like overlapping metrics).


Examples of these models are:
\begin{itemize}
    \item Encoding (create embeddings for vector databases).
    \item Clustering (discover communities inside a social networks).
    \item Outliers detection (fraud analysis).
\end{itemize}

\subsection{Generative Models}

The last category of ML models that we will present here is the one that contains the models that are trained to sample from a given probabilistic distribution. This group of models is not created by looking at how the model is trained, in fact they can be unsupervised or supervised models (to some extent). What make these models special is that they are trained to reproduce an arbitrary distribution of data that is represented in the train dataset, like human faces or Shakespeare books. The training process of these models generally have no hardcoded target metric, they often rely on adversarial training o reinforcement learning. In the adversarial training~\cite{goodfellow2020generative}, the model that is generating the outputs is guided by an other model that is trained at the same time to discriminate between generated samples and real dataset samples, leading to a Nash-Equilibrium~\cite{nash1950equilibrium} and a (hopefully) effective generator model. On the reinforcement learning setup~\cite{russell2016artificial}, a human is used in the loop to evaluate or generate preference scores of a series of model outputs, then the model is tuned to follow these preferences using a reward system. In either case, there is no exact score for a given output.

The most known generative models uses are the text-to-image and chatbots applications, examples of these are Stable Diffusion and ChatGPT respectively. When we use these models we do not seek for a fixed output, instead, we look for a creative output or an output that is built following some instructions, like "an image of a cat on a keyboard" or "Create an article that talks about the different kinds of machine learning models". Theres is not an unique answer to this commands and two completely different model outputs can be just as valid.

Measuring the effectiveness of these models on the tasks that they perform is not simple, in fact it is an open problem. Nevertheless these models are the ones that have the higher demand and are the ones that are expected to be deployed in the Pocket Network. As we commented before, there is no single metric that can correctly score all the nuances of the generated output. The only way to asses the correctness of one of these models is though multiple step measurements of different kinds of generated outputs. In practice, the common approach is to use benchmarks that include several metrics that observe the different aspects of the generated samples.


Examples of these models are:
\begin{itemize}
    \item Text-to-Image (create images that follow a given description).
    \item Language Models (complete an string using the most probable words).
    \item Text-to-Video.
    \item Text-to-Audio.
\end{itemize}


