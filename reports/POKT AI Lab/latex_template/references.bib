
@misc{liang_holistic_2023,
	title = {Holistic Evaluation of Language Models},
	url = {http://arxiv.org/abs/2211.09110},
	doi = {10.48550/arXiv.2211.09110},
	abstract = {Language models ({LMs}) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models ({HELM}) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for {LMs}. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5\% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream {LM} evaluation. Prior to {HELM}, models on average were evaluated on just 17.9\% of the core {HELM} scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for {HELM} to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
	number = {{arXiv}:2211.09110},
	publisher = {{arXiv}},
	author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and RÃ©, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
	urldate = {2024-03-12},
	date = {2023-10-01},
	eprinttype = {arxiv},
	eprint = {2211.09110 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/giskard/Zotero/storage/NWELZ2RU/Liang et al. - 2023 - Holistic Evaluation of Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/giskard/Zotero/storage/CE33BV3J/2211.html:text/html},
}

@online{poktscan_open_nodate,
	title = {{OPEN}: {POKT} {AI} Lab --- Socket - Build / Quick Grants},
	url = {https://forum.pokt.network/t/open-pokt-ai-lab-socket/5056/4},
	shorttitle = {{OPEN}},
	abstract = {Assemble with us to share ideas, help each other, and shape the future of our ecosystem.},
	titleaddon = {Pocket Network Forum},
	author = {Poktscan, Rawthil},
	urldate = {2024-03-12},
	langid = {english},
	file = {Snapshot:/home/giskard/Zotero/storage/GTXY7DVH/5056.html:text/html},
}

@misc{gao_framework_2023,
	title = {A framework for few-shot language model evaluation},
	url = {https://zenodo.org/records/10256836},
	publisher = {Zenodo},
	author = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and {DiPofi}, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and {McDonell}, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
	date = {2023-12},
	doi = {10.5281/zenodo.10256836},
	note = {Version Number: v0.4.0},
}
