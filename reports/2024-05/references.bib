@misc{biderman_lessons_2024,
	title = {Lessons from the Trenches on Reproducible Evaluation of Language Models},
	url = {http://arxiv.org/abs/2405.14782},
	doi = {10.48550/arXiv.2405.14782},
	abstract = {Effective evaluation of language models remains an open challenge in {NLP}. Researchers and engineers face methodological issues such as the sensitivity of models to evaluation setup, difficulty of proper comparisons across methods, and the lack of reproducibility and transparency. In this paper we draw on three years of experience in evaluating large language models to provide guidance and lessons for researchers. First, we provide an overview of common challenges faced in language model evaluation. Second, we delineate best practices for addressing or lessening the impact of these challenges on research. Third, we present the Language Model Evaluation Harness (lm-eval): an open source library for independent, reproducible, and extensible evaluation of language models that seeks to address these issues. We describe the features of the library as well as case studies in which the library has been used to alleviate these methodological concerns.},
	number = {{arXiv}:2405.14782},
	publisher = {{arXiv}},
	author = {Biderman, Stella and Schoelkopf, Hailey and Sutawika, Lintang and Gao, Leo and Tow, Jonathan and Abbasi, Baber and Aji, Alham Fikri and Ammanamanchi, Pawan Sasanka and Black, Sidney and Clive, Jordan and {DiPofi}, Anthony and Etxaniz, Julen and Fattori, Benjamin and Forde, Jessica Zosa and Foster, Charles and Jaiswal, Mimansa and Lee, Wilson Y. and Li, Haonan and Lovering, Charles and Muennighoff, Niklas and Pavlick, Ellie and Phang, Jason and Skowron, Aviya and Tan, Samson and Tang, Xiangru and Wang, Kevin A. and Winata, Genta Indra and Yvon, Fran√ßois and Zou, Andy},
	urldate = {2024-05-29},
	date = {2024-05-23},
	eprinttype = {arxiv},
	eprint = {2405.14782 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/giskard/Documents/back_up/Zotero/storage/KDUG7JDY/Biderman et al. - 2024 - Lessons from the Trenches on Reproducible Evaluati.pdf:application/pdf;arXiv.org Snapshot:/home/giskard/Documents/back_up/Zotero/storage/GCN32MGQ/2405.html:text/html},
}

@misc{sennrich_neural_2016,
	title = {Neural Machine Translation of Rare Words with Subword Units},
	url = {http://arxiv.org/abs/1508.07909},
	doi = {10.48550/arXiv.1508.07909},
	abstract = {Neural machine translation ({NMT}) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the {NMT} model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the {WMT} 15 translation tasks English-German and English-Russian by 1.1 and 1.3 {BLEU}, respectively.},
	number = {{arXiv}:1508.07909},
	publisher = {{arXiv}},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	urldate = {2024-05-29},
	date = {2016-06-10},
	eprinttype = {arxiv},
	eprint = {1508.07909 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/giskard/Documents/back_up/Zotero/storage/BVVS3JVT/Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf:application/pdf;arXiv.org Snapshot:/home/giskard/Documents/back_up/Zotero/storage/43KRINY9/1508.html:text/html},
}

@misc{kudo_sentencepiece_2018,
	title = {{SentencePiece}: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
	url = {http://arxiv.org/abs/1808.06226},
	doi = {10.48550/arXiv.1808.06226},
	shorttitle = {{SentencePiece}},
	abstract = {This paper describes {SentencePiece}, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, {SentencePiece} can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of {NMT} on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. {SentencePiece} is available under the Apache 2 license at https://github.com/google/sentencepiece.},
	number = {{arXiv}:1808.06226},
	publisher = {{arXiv}},
	author = {Kudo, Taku and Richardson, John},
	urldate = {2024-05-29},
	date = {2018-08-19},
	eprinttype = {arxiv},
	eprint = {1808.06226 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/giskard/Documents/back_up/Zotero/storage/8Y3C5Q74/Kudo and Richardson - 2018 - SentencePiece A simple and language independent s.pdf:application/pdf;arXiv.org Snapshot:/home/giskard/Documents/back_up/Zotero/storage/2A3SXHCV/1808.html:text/html},
}

@misc{kudo_subword_2018,
	title = {Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates},
	url = {http://arxiv.org/abs/1804.10959},
	doi = {10.48550/arXiv.1804.10959},
	shorttitle = {Subword Regularization},
	abstract = {Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation ({NMT}). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of {NMT}. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.},
	number = {{arXiv}:1804.10959},
	publisher = {{arXiv}},
	author = {Kudo, Taku},
	urldate = {2024-05-29},
	date = {2018-04-29},
	eprinttype = {arxiv},
	eprint = {1804.10959 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/giskard/Documents/back_up/Zotero/storage/XT3UJNH9/Kudo - 2018 - Subword Regularization Improving Neural Network T.pdf:application/pdf;arXiv.org Snapshot:/home/giskard/Documents/back_up/Zotero/storage/BF6QHCJI/1804.html:text/html},
}

@inproceedings{kirchenbauer2023watermark,
  title={A watermark for large language models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={17061--17084},
  year={2023},
  organization={PMLR}
}
