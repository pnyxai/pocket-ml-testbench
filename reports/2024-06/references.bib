@misc{biderman_lessons_2024,
	title = {Lessons from the Trenches on Reproducible Evaluation of Language Models},
	url = {http://arxiv.org/abs/2405.14782},
	doi = {10.48550/arXiv.2405.14782},
	abstract = {Effective evaluation of language models remains an open challenge in {NLP}. Researchers and engineers face methodological issues such as the sensitivity of models to evaluation setup, difficulty of proper comparisons across methods, and the lack of reproducibility and transparency. In this paper we draw on three years of experience in evaluating large language models to provide guidance and lessons for researchers. First, we provide an overview of common challenges faced in language model evaluation. Second, we delineate best practices for addressing or lessening the impact of these challenges on research. Third, we present the Language Model Evaluation Harness (lm-eval): an open source library for independent, reproducible, and extensible evaluation of language models that seeks to address these issues. We describe the features of the library as well as case studies in which the library has been used to alleviate these methodological concerns.},
	number = {{arXiv}:2405.14782},
	publisher = {{arXiv}},
	author = {Biderman, Stella and Schoelkopf, Hailey and Sutawika, Lintang and Gao, Leo and Tow, Jonathan and Abbasi, Baber and Aji, Alham Fikri and Ammanamanchi, Pawan Sasanka and Black, Sidney and Clive, Jordan and {DiPofi}, Anthony and Etxaniz, Julen and Fattori, Benjamin and Forde, Jessica Zosa and Foster, Charles and Jaiswal, Mimansa and Lee, Wilson Y. and Li, Haonan and Lovering, Charles and Muennighoff, Niklas and Pavlick, Ellie and Phang, Jason and Skowron, Aviya and Tan, Samson and Tang, Xiangru and Wang, Kevin A. and Winata, Genta Indra and Yvon, François and Zou, Andy},
	urldate = {2024-05-29},
	date = {2024-05-23},
	eprinttype = {arxiv},
	eprint = {2405.14782 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/giskard/Documents/back_up/Zotero/storage/KDUG7JDY/Biderman et al. - 2024 - Lessons from the Trenches on Reproducible Evaluati.pdf:application/pdf;arXiv.org Snapshot:/home/giskard/Documents/back_up/Zotero/storage/GCN32MGQ/2405.html:text/html},
}

@misc{polo_tinybenchmarks_2024,
	title = {{tinyBenchmarks}: evaluating {LLMs} with fewer examples},
	url = {http://arxiv.org/abs/2402.14992},
	shorttitle = {{tinyBenchmarks}},
	abstract = {The versatility of large language models ({LLMs}) led to the creation of diverse benchmarks that thoroughly test a variety of language models’ abilities. These benchmarks consist of tens of thousands of examples making evaluation of {LLMs} very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an {LLM} on several key benchmarks. For example, we show that to accurately estimate the performance of an {LLM} on {MMLU}, a popular multiple-choice {QA} benchmark consisting of 14K examples, it is sufficient to evaluate this {LLM} on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open {LLM} Leaderboard, {MMLU}, {HELM}, and {AlpacaEval} 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results1. (Liang et al., 2022), and {AlpacaEval} (Li et al., 2023).},
	number = {{arXiv}:2402.14992},
	publisher = {{arXiv}},
	author = {Polo, Felipe Maia and Weber, Lucas and Choshen, Leshem and Sun, Yuekai and Xu, Gongjun and Yurochkin, Mikhail},
	urldate = {2024-06-21},
	date = {2024-05-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.14992 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Polo et al. - 2024 - tinyBenchmarks evaluating LLMs with fewer example.pdf:/home/giskard/Documents/back_up/Zotero/storage/BSPCIBB4/Polo et al. - 2024 - tinyBenchmarks evaluating LLMs with fewer example.pdf:application/pdf},
}

@misc{patil_gorilla_2023,
	title = {Gorilla: Large Language Model Connected with Massive {APIs}},
	url = {http://arxiv.org/abs/2305.15334},
	shorttitle = {Gorilla},
	abstract = {Large Language Models ({LLMs}) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via {API} calls remains unfulfilled. This is a challenging task even for today's state-of-the-art {LLMs} such as {GPT}-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an {API} call. We release Gorilla, a finetuned {LLaMA}-based model that surpasses the performance of {GPT}-4 on writing {API} calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting {LLMs} directly. To evaluate the model's ability, we introduce {APIBench}, a comprehensive dataset consisting of {HuggingFace}, {TorchHub}, and {TensorHub} {APIs}. The successful integration of the retrieval system with Gorilla demonstrates the potential for {LLMs} to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu},
	number = {{arXiv}:2305.15334},
	publisher = {{arXiv}},
	author = {Patil, Shishir G. and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E.},
	urldate = {2024-06-25},
	date = {2023-05-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2305.15334 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Patil et al. - 2023 - Gorilla Large Language Model Connected with Massi.pdf:/home/giskard/Documents/back_up/Zotero/storage/Z84ECDGH/Patil et al. - 2023 - Gorilla Large Language Model Connected with Massi.pdf:application/pdf},
}

@misc{liu_improved_2024,
	title = {Improved Baselines with Visual Instruction Tuning},
	url = {http://arxiv.org/abs/2310.03744},
	abstract = {Large multimodal models ({LMM}) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of {LMMs} in a controlled setting under the {LLaVA} framework. We show that the fully-connected vision-language connector in {LLaVA} is surprisingly powerful and data-efficient. With simple modifications to {LLaVA}, namely, using {CLIP}-{ViT}-L-336px with an {MLP} projection and adding academic-task-oriented {VQA} data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ∼1 day on a single 8-A100 node. Furthermore, we present some early exploration of open problems in {LMMs}, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art {LMM} research more accessible. Code and model will be publicly available.},
	number = {{arXiv}:2310.03744},
	publisher = {{arXiv}},
	author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
	urldate = {2024-06-25},
	date = {2024-05-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2310.03744 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Liu et al. - 2024 - Improved Baselines with Visual Instruction Tuning.pdf:/home/giskard/Documents/back_up/Zotero/storage/PUMQTSG9/Liu et al. - 2024 - Improved Baselines with Visual Instruction Tuning.pdf:application/pdf},
}

@misc{yue_mmmu_2024,
	title = {{MMMU}: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert {AGI}},
	url = {http://arxiv.org/abs/2311.16502},
	shorttitle = {{MMMU}},
	abstract = {We introduce {MMMU}: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. {MMMU} includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art \& Design, Business, Science, Health \& Medicine, Humanities \& Social Science, and Tech \& Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, {MMMU} focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 14 open-source {LMMs} as well as the proprietary {GPT}-4V(ision) and Gemini highlights the substantial challenges posed by {MMMU}. Even the advanced {GPT}-4V and Gemini Ultra only achieve accuracies of 56\% and 59\% respectively, indicating significant room for improvement. We believe {MMMU} will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence.},
	number = {{arXiv}:2311.16502},
	publisher = {{arXiv}},
	author = {Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and Wei, Cong and Yu, Botao and Yuan, Ruibin and Sun, Renliang and Yin, Ming and Zheng, Boyuan and Yang, Zhenzhu and Liu, Yibo and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
	urldate = {2024-06-25},
	date = {2024-06-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2311.16502 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Yue et al. - 2024 - MMMU A Massive Multi-discipline Multimodal Unders.pdf:/home/giskard/Documents/back_up/Zotero/storage/LCJGMWB6/Yue et al. - 2024 - MMMU A Massive Multi-discipline Multimodal Unders.pdf:application/pdf},
}

@online{noauthor_open_nodate,
	title = {Open {LLM} Leaderboard - a Hugging Face Space by open-llm-leaderboard},
	url = {https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard},
	abstract = {Track, rank and evaluate open {LLMs} and chatbots},
	urldate = {2024-06-25},
	langid = {english},
	file = {Snapshot:/home/giskard/Documents/back_up/Zotero/storage/H5YS6XXA/open_llm_leaderboard.html:text/html},
}

@misc{kwon_efficient_2023,
	title = {Efficient Memory Management for Large Language Model Serving with {PagedAttention}},
	url = {http://arxiv.org/abs/2309.06180},
	doi = {10.48550/arXiv.2309.06180},
	abstract = {High throughput serving of large language models ({LLMs}) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache ({KV} cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose {PagedAttention}, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build {vLLM}, an {LLM} serving system that achieves (1) near-zero waste in {KV} cache memory and (2) flexible sharing of {KV} cache within and across requests to further reduce memory usage. Our evaluations show that {vLLM} improves the throughput of popular {LLMs} by 2-4\${\textbackslash}times\$ with the same level of latency compared to the state-of-the-art systems, such as {FasterTransformer} and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. {vLLM}'s source code is publicly available at https://github.com/vllm-project/vllm},
	number = {{arXiv}:2309.06180},
	publisher = {{arXiv}},
	author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E. and Zhang, Hao and Stoica, Ion},
	urldate = {2024-06-27},
	date = {2023-09-12},
	eprinttype = {arxiv},
	eprint = {2309.06180 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/giskard/Documents/back_up/Zotero/storage/C8SNE34N/Kwon et al. - 2023 - Efficient Memory Management for Large Language Mod.pdf:application/pdf;arXiv.org Snapshot:/home/giskard/Documents/back_up/Zotero/storage/GSQW8VBL/2309.html:text/html},
}

@inproceedings{kirchenbauer2023watermark,
  title={A watermark for large language models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={17061--17084},
  year={2023},
  organization={PMLR}
}
