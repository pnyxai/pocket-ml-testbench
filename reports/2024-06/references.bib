@misc{biderman_lessons_2024,
	title = {Lessons from the Trenches on Reproducible Evaluation of Language Models},
	url = {http://arxiv.org/abs/2405.14782},
	doi = {10.48550/arXiv.2405.14782},
	abstract = {Effective evaluation of language models remains an open challenge in {NLP}. Researchers and engineers face methodological issues such as the sensitivity of models to evaluation setup, difficulty of proper comparisons across methods, and the lack of reproducibility and transparency. In this paper we draw on three years of experience in evaluating large language models to provide guidance and lessons for researchers. First, we provide an overview of common challenges faced in language model evaluation. Second, we delineate best practices for addressing or lessening the impact of these challenges on research. Third, we present the Language Model Evaluation Harness (lm-eval): an open source library for independent, reproducible, and extensible evaluation of language models that seeks to address these issues. We describe the features of the library as well as case studies in which the library has been used to alleviate these methodological concerns.},
	number = {{arXiv}:2405.14782},
	publisher = {{arXiv}},
	author = {Biderman, Stella and Schoelkopf, Hailey and Sutawika, Lintang and Gao, Leo and Tow, Jonathan and Abbasi, Baber and Aji, Alham Fikri and Ammanamanchi, Pawan Sasanka and Black, Sidney and Clive, Jordan and {DiPofi}, Anthony and Etxaniz, Julen and Fattori, Benjamin and Forde, Jessica Zosa and Foster, Charles and Jaiswal, Mimansa and Lee, Wilson Y. and Li, Haonan and Lovering, Charles and Muennighoff, Niklas and Pavlick, Ellie and Phang, Jason and Skowron, Aviya and Tan, Samson and Tang, Xiangru and Wang, Kevin A. and Winata, Genta Indra and Yvon, François and Zou, Andy},
	urldate = {2024-05-29},
	date = {2024-05-23},
	eprinttype = {arxiv},
	eprint = {2405.14782 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/giskard/Documents/back_up/Zotero/storage/KDUG7JDY/Biderman et al. - 2024 - Lessons from the Trenches on Reproducible Evaluati.pdf:application/pdf;arXiv.org Snapshot:/home/giskard/Documents/back_up/Zotero/storage/GCN32MGQ/2405.html:text/html},
}

@misc{polo_tinybenchmarks_2024,
	title = {{tinyBenchmarks}: evaluating {LLMs} with fewer examples},
	url = {http://arxiv.org/abs/2402.14992},
	shorttitle = {{tinyBenchmarks}},
	abstract = {The versatility of large language models ({LLMs}) led to the creation of diverse benchmarks that thoroughly test a variety of language models’ abilities. These benchmarks consist of tens of thousands of examples making evaluation of {LLMs} very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an {LLM} on several key benchmarks. For example, we show that to accurately estimate the performance of an {LLM} on {MMLU}, a popular multiple-choice {QA} benchmark consisting of 14K examples, it is sufficient to evaluate this {LLM} on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open {LLM} Leaderboard, {MMLU}, {HELM}, and {AlpacaEval} 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results1. (Liang et al., 2022), and {AlpacaEval} (Li et al., 2023).},
	number = {{arXiv}:2402.14992},
	publisher = {{arXiv}},
	author = {Polo, Felipe Maia and Weber, Lucas and Choshen, Leshem and Sun, Yuekai and Xu, Gongjun and Yurochkin, Mikhail},
	urldate = {2024-06-21},
	date = {2024-05-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.14992 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Polo et al. - 2024 - tinyBenchmarks evaluating LLMs with fewer example.pdf:/home/giskard/Documents/back_up/Zotero/storage/BSPCIBB4/Polo et al. - 2024 - tinyBenchmarks evaluating LLMs with fewer example.pdf:application/pdf},
}

@misc{patil_gorilla_2023,
	title = {Gorilla: Large Language Model Connected with Massive {APIs}},
	url = {http://arxiv.org/abs/2305.15334},
	shorttitle = {Gorilla},
	abstract = {Large Language Models ({LLMs}) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via {API} calls remains unfulfilled. This is a challenging task even for today's state-of-the-art {LLMs} such as {GPT}-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an {API} call. We release Gorilla, a finetuned {LLaMA}-based model that surpasses the performance of {GPT}-4 on writing {API} calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting {LLMs} directly. To evaluate the model's ability, we introduce {APIBench}, a comprehensive dataset consisting of {HuggingFace}, {TorchHub}, and {TensorHub} {APIs}. The successful integration of the retrieval system with Gorilla demonstrates the potential for {LLMs} to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu},
	number = {{arXiv}:2305.15334},
	publisher = {{arXiv}},
	author = {Patil, Shishir G. and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E.},
	urldate = {2024-06-25},
	date = {2023-05-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2305.15334 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Patil et al. - 2023 - Gorilla Large Language Model Connected with Massi.pdf:/home/giskard/Documents/back_up/Zotero/storage/Z84ECDGH/Patil et al. - 2023 - Gorilla Large Language Model Connected with Massi.pdf:application/pdf},
}

@misc{liu_improved_2024,
	title = {Improved Baselines with Visual Instruction Tuning},
	url = {http://arxiv.org/abs/2310.03744},
	abstract = {Large multimodal models ({LMM}) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of {LMMs} in a controlled setting under the {LLaVA} framework. We show that the fully-connected vision-language connector in {LLaVA} is surprisingly powerful and data-efficient. With simple modifications to {LLaVA}, namely, using {CLIP}-{ViT}-L-336px with an {MLP} projection and adding academic-task-oriented {VQA} data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ∼1 day on a single 8-A100 node. Furthermore, we present some early exploration of open problems in {LMMs}, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art {LMM} research more accessible. Code and model will be publicly available.},
	number = {{arXiv}:2310.03744},
	publisher = {{arXiv}},
	author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
	urldate = {2024-06-25},
	date = {2024-05-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2310.03744 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Liu et al. - 2024 - Improved Baselines with Visual Instruction Tuning.pdf:/home/giskard/Documents/back_up/Zotero/storage/PUMQTSG9/Liu et al. - 2024 - Improved Baselines with Visual Instruction Tuning.pdf:application/pdf},
}

@misc{yue_mmmu_2024,
	title = {{MMMU}: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert {AGI}},
	url = {http://arxiv.org/abs/2311.16502},
	shorttitle = {{MMMU}},
	abstract = {We introduce {MMMU}: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. {MMMU} includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art \& Design, Business, Science, Health \& Medicine, Humanities \& Social Science, and Tech \& Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, {MMMU} focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 14 open-source {LMMs} as well as the proprietary {GPT}-4V(ision) and Gemini highlights the substantial challenges posed by {MMMU}. Even the advanced {GPT}-4V and Gemini Ultra only achieve accuracies of 56\% and 59\% respectively, indicating significant room for improvement. We believe {MMMU} will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence.},
	number = {{arXiv}:2311.16502},
	publisher = {{arXiv}},
	author = {Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and Wei, Cong and Yu, Botao and Yuan, Ruibin and Sun, Renliang and Yin, Ming and Zheng, Boyuan and Yang, Zhenzhu and Liu, Yibo and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
	urldate = {2024-06-25},
	date = {2024-06-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2311.16502 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Yue et al. - 2024 - MMMU A Massive Multi-discipline Multimodal Unders.pdf:/home/giskard/Documents/back_up/Zotero/storage/LCJGMWB6/Yue et al. - 2024 - MMMU A Massive Multi-discipline Multimodal Unders.pdf:application/pdf},
}

@online{noauthor_open_nodate,
	title = {Open {LLM} Leaderboard - a Hugging Face Space by open-llm-leaderboard},
	url = {https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard},
	abstract = {Track, rank and evaluate open {LLMs} and chatbots},
	urldate = {2024-06-25},
	langid = {english},
	file = {Snapshot:/home/giskard/Documents/back_up/Zotero/storage/H5YS6XXA/open_llm_leaderboard.html:text/html},
}

@misc{kwon_efficient_2023,
	title = {Efficient Memory Management for Large Language Model Serving with {PagedAttention}},
	url = {http://arxiv.org/abs/2309.06180},
	doi = {10.48550/arXiv.2309.06180},
	abstract = {High throughput serving of large language models ({LLMs}) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache ({KV} cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose {PagedAttention}, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build {vLLM}, an {LLM} serving system that achieves (1) near-zero waste in {KV} cache memory and (2) flexible sharing of {KV} cache within and across requests to further reduce memory usage. Our evaluations show that {vLLM} improves the throughput of popular {LLMs} by 2-4\${\textbackslash}times\$ with the same level of latency compared to the state-of-the-art systems, such as {FasterTransformer} and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. {vLLM}'s source code is publicly available at https://github.com/vllm-project/vllm},
	number = {{arXiv}:2309.06180},
	publisher = {{arXiv}},
	author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E. and Zhang, Hao and Stoica, Ion},
	urldate = {2024-06-27},
	date = {2023-09-12},
	eprinttype = {arxiv},
	eprint = {2309.06180 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/giskard/Documents/back_up/Zotero/storage/C8SNE34N/Kwon et al. - 2023 - Efficient Memory Management for Large Language Mod.pdf:application/pdf;arXiv.org Snapshot:/home/giskard/Documents/back_up/Zotero/storage/GSQW8VBL/2309.html:text/html},
}

@inproceedings{kirchenbauer2023watermark,
  title={A watermark for large language models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={17061--17084},
  year={2023},
  organization={PMLR}
}

@article{Clark2018ThinkYH,
  title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.05457}
}

@inproceedings{zellers2019hellaswag,
    title={HellaSwag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year={2019}
}

@inproceedings{lin-etal-2022-truthfulqa,
    title = "{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods",
    author = "Lin, Stephanie  and
      Hilton, Jacob  and
      Evans, Owain",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.229",
    doi = "10.18653/v1/2022.acl-long.229",
    pages = "3214--3252",
}

@article{sakaguchi2019winogrande,
    title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
    author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
    journal={arXiv preprint arXiv:1907.10641},
    year={2019}
}

@misc{cobbe2021training,
      title={Training Verifiers to Solve Math Word Problems},
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hendrycks_measuring_2021,
	title = {Measuring Massive Multitask Language Understanding},
	url = {http://arxiv.org/abs/2009.03300},
	abstract = {We propose a new test to measure a text model’s multitask accuracy. The test covers 57 tasks including elementary mathematics, {US} history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We ﬁnd that while most recent models have near random-chance accuracy, the very largest {GPT}-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have nearrandom accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model’s academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	number = {{arXiv}:2009.03300},
	publisher = {{arXiv}},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	urldate = {2024-07-01},
	date = {2021-01-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2009.03300 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf:/home/giskard/Documents/back_up/Zotero/storage/TAU78G6Q/Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf:application/pdf},
}


@misc{chen_how_2023,
	title = {How is {ChatGPT}'s behavior changing over time?},
	url = {http://arxiv.org/abs/2307.09009},
	abstract = {{GPT}-3.5 and {GPT}-4 are the two most widely used large language model ({LLM}) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of {GPT}-3.5 and {GPT}-4 on several diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3) opinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating code, 6) {US} Medical License tests, and 7) visual reasoning. We find that the performance and behavior of both {GPT}-3.5 and {GPT}-4 can vary greatly over time. For example, {GPT}-4 (March 2023) was reasonable at identifying prime vs. composite numbers (84\% accuracy) but {GPT}-4 (June 2023) was poor on these same questions (51\% accuracy). This is partly explained by a drop in {GPT}-4’s amenity to follow chain-of-thought prompting. Interestingly, {GPT}-3.5 was much better in June than in March in this task. {GPT}-4 became less willing to answer sensitive questions and opinion survey questions in June than in March. {GPT}-4 performed better at multi-hop questions in June than in March, while {GPT}-3.5’s performance dropped on this task. Both {GPT}-4 and {GPT}-3.5 had more formatting mistakes in code generation in June than in March. We provide evidence that {GPT}-4’s ability to follow user instructions has decreased over time, which is one common factor behind the many behavior drifts. Overall, our findings show that the behavior of the “same” {LLM} service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of {LLMs}.},
	number = {{arXiv}:2307.09009},
	publisher = {{arXiv}},
	author = {Chen, Lingjiao and Zaharia, Matei and Zou, James},
	urldate = {2024-07-01},
	date = {2023-10-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2307.09009 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Chen et al. - 2023 - How is ChatGPT's behavior changing over time.pdf:/home/giskard/Documents/back_up/Zotero/storage/XA5ESP3Y/Chen et al. - 2023 - How is ChatGPT's behavior changing over time.pdf:application/pdf},
}


@misc{zhang_careful_2024,
	title = {A Careful Examination of Large Language Model Performance on Grade School Arithmetic},
	url = {http://arxiv.org/abs/2405.00332},
	doi = {10.48550/arXiv.2405.00332},
	abstract = {Large language models ({LLMs}) have achieved impressive success on many benchmarks for mathematical reasoning. However, there is growing concern that some of this performance actually reflects dataset contamination, where data closely resembling benchmark questions leaks into the training data, instead of true reasoning ability. To investigate this claim rigorously, we commission Grade School Math 1000 ({GSM}1k). {GSM}1k is designed to mirror the style and complexity of the established {GSM}8k benchmark, the gold standard for measuring elementary mathematical reasoning. We ensure that the two benchmarks are comparable across important metrics such as human solve rates, number of steps in solution, answer magnitude, and more. When evaluating leading open- and closed-source {LLMs} on {GSM}1k, we observe accuracy drops of up to 13\%, with several families of models (e.g., Phi and Mistral) showing evidence of systematic overfitting across almost all model sizes. At the same time, many models, especially those on the frontier, (e.g., Gemini/{GPT}/Claude) show minimal signs of overfitting. Further analysis suggests a positive relationship (Spearman's r{\textasciicircum}2=0.32) between a model's probability of generating an example from {GSM}8k and its performance gap between {GSM}8k and {GSM}1k, suggesting that many models may have partially memorized {GSM}8k.},
	number = {{arXiv}:2405.00332},
	publisher = {{arXiv}},
	author = {Zhang, Hugh and Da, Jeff and Lee, Dean and Robinson, Vaughn and Wu, Catherine and Song, Will and Zhao, Tiffany and Raja, Pranav and Slack, Dylan and Lyu, Qin and Hendryx, Sean and Kaplan, Russell and Lunati, Michele and Yue, Summer},
	urldate = {2024-07-01},
	date = {2024-05-03},
	eprinttype = {arxiv},
	eprint = {2405.00332 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/giskard/Documents/back_up/Zotero/storage/G4PAK6Q5/Zhang et al. - 2024 - A Careful Examination of Large Language Model Perf.pdf:application/pdf;arXiv.org Snapshot:/home/giskard/Documents/back_up/Zotero/storage/WFM68IVV/2405.html:text/html},
}

@misc{golchin_data_2024,
	title = {Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models},
	url = {http://arxiv.org/abs/2311.06233},
	doi = {10.48550/arXiv.2311.06233},
	shorttitle = {Data Contamination Quiz},
	abstract = {We propose the Data Contamination Quiz ({DCQ}), a simple and effective approach to detect data contamination in large language models ({LLMs}) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each subsampled instance from a specific dataset partition (e.g., {GSM}8k test set) are created. These changes only include word-level perturbations. The generated perturbations, along with the original dataset instance, form the options in the {DCQ}, with an extra option accommodating the possibility of selecting none of the provided options. Given that the only distinguishing signal among the options is the exact wording with respect to the original dataset instance, an {LLM}, when tasked with identifying the original dataset instance, gravitates towards selecting the original one if it has been exposed to it in its pre-training phase -- a trait intrinsic to {LLMs}. While accounting for positional biases in {LLMs}, the quiz performance reveals the contamination level for the model being examined with the dataset partition to which the quiz pertains. Applied to various datasets with {GPT}-4 and {GPT}-3.5, our findings -- while fully lacking access to pre-training data and model parameters -- suggest that {DCQ} achieves state-of-the-art results and uncovers greater contamination/memorization levels compared to existing methods and proficiently bypasses more safety filters, especially those set to avoid generating copyrighted contents.},
	number = {{arXiv}:2311.06233},
	publisher = {{arXiv}},
	author = {Golchin, Shahriar and Surdeanu, Mihai},
	urldate = {2024-07-01},
	date = {2024-05-24},
	eprinttype = {arxiv},
	eprint = {2311.06233 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/giskard/Documents/back_up/Zotero/storage/F5M9ICVX/Golchin and Surdeanu - 2024 - Data Contamination Quiz A Tool to Detect and Esti.pdf:application/pdf;arXiv.org Snapshot:/home/giskard/Documents/back_up/Zotero/storage/YMFINVSS/2311.html:text/html},
}

@misc{xu_benchmarking_2024,
	title = {Benchmarking Benchmark Leakage in Large Language Models},
	url = {http://arxiv.org/abs/2404.18824},
	doi = {10.48550/arXiv.2404.18824},
	abstract = {Amid the expanding use of pre-training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion of supervised data in contemporary Large Language Models ({LLMs}). This issue skews benchmark effectiveness and fosters potentially unfair comparisons, impeding the field's healthy development. To address this, we introduce a detection pipeline utilizing Perplexity and N-gram accuracy, two simple and scalable metrics that gauge a model's prediction precision on benchmark, to identify potential data leakages. By analyzing 31 {LLMs} under the context of mathematical reasoning, we reveal substantial instances of training even test set misuse, resulting in potentially unfair comparisons. These findings prompt us to offer several recommendations regarding model documentation, benchmark setup, and future evaluations. Notably, we propose the "Benchmark Transparency Card" to encourage clear documentation of benchmark utilization, promoting transparency and healthy developments of {LLMs}. we have made our leaderboard, pipeline implementation, and model predictions publicly available, fostering future research.},
	number = {{arXiv}:2404.18824},
	publisher = {{arXiv}},
	author = {Xu, Ruijie and Wang, Zengzhi and Fan, Run-Ze and Liu, Pengfei},
	urldate = {2024-07-01},
	date = {2024-04-29},
	eprinttype = {arxiv},
	eprint = {2404.18824 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/giskard/Documents/back_up/Zotero/storage/PI32MU5G/Xu et al. - 2024 - Benchmarking Benchmark Leakage in Large Language M.pdf:application/pdf;arXiv.org Snapshot:/home/giskard/Documents/back_up/Zotero/storage/WGD42WAL/2404.html:text/html},
}

@misc{balloccu_leak_2024,
	title = {Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source {LLMs}},
	url = {http://arxiv.org/abs/2402.03927},
	doi = {10.48550/arXiv.2402.03927},
	shorttitle = {Leak, Cheat, Repeat},
	abstract = {Natural Language Processing ({NLP}) research is increasingly focusing on the use of Large Language Models ({LLMs}), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of {\textbackslash}emph\{indirect\} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using {OpenAI}'s {GPT}-3.5 and {GPT}-4, the most prominently used {LLMs} today, in the context of data contamination. By analysing 255 papers and considering {OpenAI}'s data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been globally exposed to \${\textbackslash}sim\$4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues. We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts.},
	number = {{arXiv}:2402.03927},
	publisher = {{arXiv}},
	author = {Balloccu, Simone and Schmidtová, Patrícia and Lango, Mateusz and Dušek, Ondřej},
	urldate = {2024-07-01},
	date = {2024-02-22},
	eprinttype = {arxiv},
	eprint = {2402.03927 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/giskard/Documents/back_up/Zotero/storage/SS898ZEQ/Balloccu et al. - 2024 - Leak, Cheat, Repeat Data Contamination and Evalua.pdf:application/pdf;arXiv.org Snapshot:/home/giskard/Documents/back_up/Zotero/storage/PD5D3SIC/2402.html:text/html},
}
