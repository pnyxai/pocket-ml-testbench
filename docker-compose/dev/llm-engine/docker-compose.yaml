x-base: &base
  restart: unless-stopped
  extra_hosts:
    # this allows containers to call a port on the docker host machine
    # instead of localhost (because that will be container) use host.docker.internal
    # to resolve the docker host IP dynamically
    host.docker.internal: host-gateway
  networks:
    - llm-engine-net
    - morse-localnet
  logging:
    options:
      mode: non-blocking
      max-size: "10m"
      max-file: "3"


services:
  sidecar:
    <<: *base
    build:
      context: ../../
      dockerfile: apps/python/sidecar/Dockerfile
    image: pocket_ml_sidecar:dev
    deploy:
      replicas: 1
    environment:
      CONFIG_PATH: /home/app/configs/config.json
    volumes:
      - $SIDECAR_CONFIG_FILE:/home/app/configs/config.json
      - $SIDECAR_TOKENIZER_FILE:/tokenizer/

  nginx-sidecar:
    <<: *base
    image: nginx:latest
    container_name: nginx-sidecar
    ports:
      - "9087:9087"
    volumes:
      - $SIDECAR_NGINX_CONFIG_FILE:/etc/nginx/nginx.conf:ro
    depends_on:
      - sidecar

  llm-engine:
    <<: *base
    container_name: llm-engine
    image: vllm/vllm-openai:${VLLM_VERSION}
    healthcheck:
      test: [ "CMD-SHELL", "python3", "/root/healthcheck.py" ]
    volumes:
      - ${MODELS_PATH}:/root/.cache/huggingface/hub/
      - ./healthcheck.py:/root/healthcheck.py
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - MODEL_NAME=${CHAT_MODEL_NAME}
      - NUM_GPUS=${NUM_GPUS}
      - QUANTIZATION=${QUANTIZATION}
      - GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION}
      - MAX_MODEL_LEN=${MAX_MODEL_LEN}
      - MAX_NUM_SEQS=${MAX_NUM_SEQS}
      - DTYPE=${DTYPE}
    entrypoint: ["python3",
      "-m",
      "vllm.entrypoints.openai.api_server",
      "--model",
      "${CHAT_MODEL_NAME}",
      "--tensor-parallel-size",
      "${NUM_GPUS}",
      "--quantization",
      "${QUANTIZATION}",
      "--dtype",
      "${DTYPE}",
      "--gpu-memory-utilization",
      "${GPU_MEMORY_UTILIZATION}",
      "--max-model-len",
      "${MAX_MODEL_LEN}",
      "--served-model-name",
      "${SERVED_MODEL_NAME}",
      "--trust-remote-code",
      "--max-num-seqs",
      "${MAX_NUM_SEQS}",
      "--max-num-batched-tokens",
      "${MAX_MODEL_LEN}",
      ]
    ports:
     - "9900:8000"
    shm_size: '4gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['${GPU_ID}']
              capabilities: [gpu]


networks:
  llm-engine-net:
    name: llm-engine-net
    driver: bridge
  morse-localnet:
    name: morse-localnet
    external: true