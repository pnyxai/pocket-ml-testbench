version: '3'
services:
  vllm-openai-main:
    container_name: vllm-openai-main
    image: vllm/vllm-openai:v0.3.2
    volumes:
      - ${MODELS_PATH}:/root/.cache/huggingface/hub/
      - ${CHAT_TEMPLATE_FILE}:/workspace/chat_template.jinja2
      - ${TOOLS_TEMPLATE_FILE}:/workspace/tools_template.jinja
    environment:
      - MODEL_NAME=${MODEL_NAME}
      - NUM_GPUS=${NUM_GPUS}
      - QUANTIZATION=${QUANTIZATION}
      - GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION}
      - MAX_MODEL_LEN=${MAX_MODEL_LEN}
      - MAX_NUM_SEQS=${MAX_NUM_SEQS}
    entrypoint: ["python3",
      "-m",
      "vllm.entrypoints.openai.api_server",
      "--model",
      "${MODEL_NAME}",
      "--tensor-parallel-size",
      "${NUM_GPUS}",
     "--quantization",
     "${QUANTIZATION}",
      "--dtype",
      "auto",
      "--gpu-memory-utilization",
      "${GPU_MEMORY_UTILIZATION}",
      "--max-model-len",
      "${MAX_MODEL_LEN}",
      "--chat-template",
      "chat_template.jinja2",
      "--served-model-name",
      "${SERVED_MODEL_NAME}",
      "--trust-remote-code",
      "--max-num-seqs",
      "${MAX_NUM_SEQS}",
      ]
    ports:
     - "8000:8000"
    shm_size: '4gb' # This is only used for GPU parallelism
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0'] # Add more if you have more, remember to update the .env for GPU parallelism
              capabilities: [gpu]